{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Machine Learning Tutorial](https://github.com/wikistat/MLTraining): IoT and Human Activity Recognition (HAR)\n",
    "## Analyse  de signaux  issus d'un *smartphone* \n",
    "## Utilisation des librairies <a href=\"http://scikit-learn.org/stable/#\"><img src=\"http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\" style=\"max-width: 100px; display: inline\" alt=\"Scikit-Learn\"/></a> en <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 120px; display: inline\" alt=\"Python\"/></a> et <a href=\"https://keras.io/\"><img src=\"https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png\" style=\"max-width: 100px; display: inline\" alt=\"Keras\"/></a> \n",
    "\n",
    "\n",
    "### Résumé\n",
    "Cas d'usage de [reconnaissance d'activités humaines](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) à partir des enregistrements de signaux (gyroscope, accéléromètre) issus d'un *objet connecté*: un simple smartphone. Les données sont analysées pour illustrer les principales étapes communes en *science des données* et appliquables à des signaux physiques échantillonnés. Visualisation des signaux bruts afin d'évaluer les difficultés posées par ce type de données; exploration ([analyse en composantes principales](http://wikistat.fr/pdf/st-m-explo-acp.pdf), [analyse factorielle discriminante](http://wikistat.fr/pdf/st-m-explo-acp.pdf)) des données transformées (*features*) ou *métier* calculées à partir des signaux; prévision de l'activité à partir des données métier par la plupart des méthodes linéaires dont: [régression logistique](http://wikistat.fr/pdf/st-m-app-rlogit.pdf), [SVM](http://wikistat.fr/pdf/st-m-app-svm.pdf) et non linéaires; prévision de l'activité à partir des signaux brutes par [réseau de neurones](http://wikistat.fr/pdf/st-m-app-rn.pdf) élémentaire puis [réseau convolutionnel](http://wikistat.fr/pdf/st-m-app-rn.pdf) (*deep learning*). Ce calepin montre les très bonnes qualités (96%) de prévision des méthodes élémentaires (linéaires) sur les données métier puis, pour économiser les coûteuses (pour la batterie embarquée) transformations, des qualités similaires de prévision  sont obtenues par un réseau convolutionnel sur les signaux bruts. Enfin ces mêmes données sont utilisées pour illustrer les techniques de [détection d'anomalies](http://wikistat.fr/pdf/st-m-app-anomalies.pdf). Celles-ci sont indispensables, comme un bouclier pour détecter des cas ou situations n'apparaissant pas dans la base d'apprentisage et qui pourraient donc conduire logiquement à des erreurs de prévision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {} ,
   "source": [
    "##   Introduction\n",
    "###   Objectif général\n",
    "L'objectif est de reconnaître l'activité d'un individu porteur d'un smartphone qui enregistre un ensemble de signaux issu du gyroscope et de l'accéléromètre embarqués et ainsi connectés. Une base de données d'apprentissage a été construite expérimentalement. Un ensemble de porteurs d'un smartphone ont produit une activité déterminée pendant un laps de temps prédéfini tandis que des signaux étaient enregistrés. Les données sont issues de la communauté qui vise la reconnaissance d'activités humaines (*Human activity recognition, HAR*). Voir à ce propos l'[article](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2013-11.pdf) relatant un colloque de 2013.  L'analyse des données associée à une identification d'activité en temps réel, ne sont pas abordées.\n",
    "\n",
    "Les données publiques disponibles ont été acquises, décrites et partièlement analysées par [Anguita et al. (2013)](https://www.icephd.org/sites/default/files/IWAAL2012.pdf). Elles sont accessibles sur le [dépôt](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) de l'University California Irvine (UCI) consacré à l'apprentissage machine.\n",
    "\n",
    "L'archive contient les données brutes: accélérations échantillonnnées à 64 htz pendant 2s. Les accélérations en x, y, et z, chacune de 128 colonnes, celles en y soustrayant la gravité naturelle ainsi que les accélérations angulaires (gyroscope) en x, y, et z soit en tout 9 fichiers. Le choix d'une puissance de 2 pour la fréquence d'échantillonnage permet l'exécution efficace d'algorithmes de transformée de Fourier ou en ondelettes.\n",
    "\n",
    "\n",
    "###  Déroulement\n",
    "Une première visualisation et exploration des signaux bruts montre (section 2) que ceux-ci sont difficiles à analyser; les classes d'activité y sont en effet mal caractérisées. La principale cause est l'absence de synchronisation des débuts d'activité; le déphasage des signaux apparaît alors comme un bruit ou artefact très préjudiciable à la bonne discrimination des activités sur la base d'une distance euclidienne usuelle ($L_2$). C'est la raison pour laquelle, [Anguita et al. (2013)](https://www.icephd.org/sites/default/files/IWAAL2012.pdf) proposent de calculer un ensemble de transformations ou caractéristiques (*features*) classiques du traitement du signal: variance, corrélations, entropie, décompositions de Fourier... Ce sont alors $p=561$ variables qui sont considérées et explorées dans la section 3. L'[analyse en composantes principales](http://wikistat.fr/pdf/st-m-explo-acp.pdf) et surtout l'[analyse factorielle discriminante](http://wikistat.fr/pdf/st-m-explo-acp.pdf) montrent les bonnes qualités discriminatoires de ces données \"métier\" issues d'une connaissance experte des signaux. La section 4 exploite ces variables métier et montre que des modèles statistiques élémentaires car linéaires (régression logistique, analyse discriminante) ou qu'un algorithme classique de machine à vecteurs supports (SVM) utilisant un simple noyau linéaire conduit à d'excellentes prévisions au contraire d'algorihtmes non linaires sophistiqués (*random forest, gradient boosting*).\n",
    "\n",
    "Néanmoins, faire calculer en permanence des transformations sophistiquées (Fourier) n'est pas une solution viable pour la batterie d'un objet embarqué connecté. L'algorithme candidat doit pouvoir produire une solution intégrable (cablée) dans un cicuit, comme c'est par exemple le cas des puces dédiées à la reconnaissance faciale. C'est l'objet de la section 5: montrer la faisabilité d'une solution basée sur les seuls signaux brutes; solution mettant en oeuvre un réseau de neurones intégrant une [couche convolutionnelle](http://wikistat.fr/pdf/st-m-app-rn.pdf). \n",
    "\n",
    "Enfin, la section 6, qui peut être exécutée indépendamment des précédentes, est consacrée à l'illustration des techniques de [détection d'anomalies](http://wikistat.fr/pdf/st-m-app-anomalies.pdf). Le déploiement de ces outils est indispensble durant toute la durée de vie d'un système d'intelligence artificielle basé sur un ou des algorithmes d'apprentissage machine et donc sur une base de données d'entraînement. Cette dernière doit être évidemment représentative des situations que le système va rencontrer mais il est fort probable que toutes n'y soient pas documentées. Aussi, en mode opérationnel, il est fondamental de pouvoir anticiper la détection d'une situation hors classe ou hors norme, non réprésentée dans la base, et pour la quelle la prévision ne peut être qu'hasardeuse. C'est un gage de la confiance qui peut être accordée aux précisions des prévisions.\n",
    "\n",
    "###  Environnement logiciel\n",
    "Pour être exécuté, ce calepin (*jupyter notebook*) nécessite l'installation de Python3 via par exemple le site  [Anaconda](https://conda.io/docs/user-guide/install/download.html). Les algorihtmes d'exploration et d'apprentissage statistiques utilisés sont disponibles dans la librairie [`Scikit-learn`](http://scikit-learn.org/stable/) tandis qu'une approche élémentaire de l'apprentissage profond des réseaux de neurones avec couche convolutionnelle nécessite l'installation de la librairie [`Keras`](https://keras.io/) qui entraine celle de [`TensorFlow`](https://www.tensorflow.org/). \n",
    "\n",
    "*Remarques*: \n",
    "- ce calepin a été construit et testé sous Ubuntu Mate 16.04 (Python 3.8) mais son utilisation sous Windows ou Mac OS ne devrait pas poser de problème une fois l'environnement correctement installé;\n",
    "- la commande `conda` installe sans difficulté l'environnement `Keras` en incluant `TensorFlow`;\n",
    "- les réseaux de neurones considérés restent de structure simple, une carte GPU n'est pas indispensable à leur apprentissage sauf si l'utilisateur souhaite approfonfir les optimisations et choix de couches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Episode 1</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Etude préalable des signaux bruts\n",
    "###  Source\n",
    "\n",
    "Les données sont celles originales du dépôt de l'[UCI](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones). Elle doivent être préalablement téléchargées en cliquant [ici](https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip).\n",
    "\n",
    "Chaque enregistrement ou unité statistique ou instance est labellisée avec **6 activités**: debout, assis, couché, marche, monter ou descendre un escalier. Chaque jeu de données est partagé en une partie échantillon d'apprentissage et une partie échantillon test. L'échantillon test n'est utilisé que pour évaluer et comparer les qualités de prévision des principales méthodes. Il est conservé en l'état afin de rendre les comparaisons possibles avec les résultats de la littérature. Il s'agit donc d'un problème de *classification supervisée* (6 classes) avec $n=10299$ échantillons pour l'apprentissage, 2947 pour le test.\n",
    "\n",
    "Les données contiennent deux jeux de dimensions différentes:\n",
    "\n",
    "1. Jeu multidimensionel: un individus est constitué de 9 Séries Temporelles de *dimensions* $(n, 128, 9)$\n",
    "2. Jeu unidimensionnel: Les 9 Séries Temporelles sont concaténées pour constituer un vecteur de 128*9 = 1152 variables de *dimensions* $(n, 1152)$\n",
    "\n",
    "*N.B.* La structure des données est nettement plus complexe que celles couramment étudiées dans le [dépôt Wikistat](https://github.com/wikistat/). Le code a été structuré en une séquence de fonctions afin d'en faciliter la compréhension. L'outil *calepin* atteint ici des limites pour la réalisation de codes complexes.\n",
    "\n",
    "\n",
    "### Importation des principales librairies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:21.403179Z",
     "start_time": "2020-04-14T09:09:20.486909Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Structurer les données\n",
    "Définir le chemin d'accès aux données puis les fonctions utiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:21.419530Z",
     "start_time": "2020-04-14T09:09:21.405490Z"
    }
   },
   "outputs": [],
   "source": [
    "# Attention: le chemin ci-dessous doit être adapté au contexte\n",
    "DATADIR_UCI = './UCI HAR Dataset'\n",
    "# Liste des noms des fichiers afin d'automatiser la lecture.\n",
    "SIGNALS = [ \"body_acc_x\", \"body_acc_y\", \"body_acc_z\", \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\", \"total_acc_x\", \"total_acc_y\", \"total_acc_z\"]\n",
    "\n",
    "# Fonctions permettant de lire la séquence des fichiers avant de restructurer les données \n",
    "# dans le fortmat recherché.\n",
    "def my_read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "def load_signal(data_dir, subset, signal):\n",
    "    filename = data_dir+'/'+subset+'/Inertial Signals/'+signal+'_'+subset+'.txt'\n",
    "    x = my_read_csv(filename).values\n",
    "    return x \n",
    "\n",
    "def load_signals(data_dir, subset, flatten = False):\n",
    "    signals_data = []\n",
    "    for signal in SIGNALS:\n",
    "        signals_data.append(load_signal(data_dir, subset, signal)) \n",
    "    if flatten :\n",
    "        X = np.hstack(signals_data)\n",
    "    else:\n",
    "        X = np.transpose(signals_data, (1, 2, 0))    \n",
    "    return X \n",
    "\n",
    "def load_y(data_dir, subset, dummies = False):\n",
    "    filename = data_dir+'/'+subset+'/y_'+subset+'.txt'\n",
    "    y = my_read_csv(filename)[0]\n",
    "    if dummies:\n",
    "        Y = pd.get_dummies(y).values\n",
    "    else:\n",
    "        Y = y.values\n",
    "    return Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:31.563886Z",
     "start_time": "2020-04-14T09:09:21.423726Z"
    }
   },
   "outputs": [],
   "source": [
    "#Multidimensional Data\n",
    "X_train, X_test = load_signals(DATADIR_UCI, 'train'), load_signals(DATADIR_UCI, 'test')\n",
    "# Flattened Data\n",
    "X_train_flatten, X_test_flatten = load_signals(DATADIR_UCI, 'train', flatten=True), load_signals(DATADIR_UCI, 'test', flatten=True)\n",
    "\n",
    "# Label Y\n",
    "Y_train_label, Y_test_label = load_y(DATADIR_UCI, 'train', dummies = False), load_y(DATADIR_UCI, 'test', dummies = False)\n",
    "#Dummies Y (For Keras)\n",
    "Y_train_dummies, Y_test_dummies = load_y(DATADIR_UCI, 'train', dummies = True), load_y(DATADIR_UCI, 'test', dummies = True)\n",
    "\n",
    "N_train = X_train.shape[0]\n",
    "N_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification des dimensions afin de s'assurer de la bonne lecture des fichiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:31.579575Z",
     "start_time": "2020-04-14T09:09:31.566388Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Dimension\")\n",
    "print(\"Données Multidimensionelles, : \" + str(X_train.shape))\n",
    "print(\"Données Unimensionelles, : \" + str(X_train_flatten.shape))\n",
    "print(\"Vecteur réponse (scikit-learn) : \" + str(Y_train_label.shape))\n",
    "print(\"Matrice réponse(Keras) : \" + str(Y_train_dummies.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Visualisations\n",
    "Certre phase est essentielle à la bonne compréhension des données, de leur structure et donc des problèmes qui vont être soulevés par la suite. La visualisation est très élémentaire d'un point de vue méthodologique mais nécessite des compétences plus éléborées en Python et donc des \n",
    "#### Fonctions utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:31.609094Z",
     "start_time": "2020-04-14T09:09:31.584773Z"
    }
   },
   "outputs": [],
   "source": [
    "# Liste des couleurs\n",
    "CMAP = plt.get_cmap(\"Accent\")\n",
    "# Liste des types de signaux\n",
    "SIGNALS = [\"body_acc x\", \"body_acc y\", \"body_acc z\", \n",
    "                \"body_gyro x\", \"body_gyro y\", \"body_gyro z\", \n",
    "               \"total_acc x\", \"total_acc y\", \"total_acc z\"] \n",
    "# Dictionnaire en clair des activités expérimentées (contexte supervisé)\n",
    "ACTIVITY_DIC = {1 : \"WALKING\",\n",
    "2 : \"WALKING UPSTAIRS\",\n",
    "3 : \"WALKING DOWNSTAIRS\",\n",
    "4 : \"SITTING\",\n",
    "5 : \"STANDING\",\n",
    "6 : \"LAYING\"}\n",
    "labels = ACTIVITY_DIC.values()\n",
    "\n",
    "# Fonction pour le tracé d'un signal\n",
    "def plot_one_axe(X, fig, ax, sample_to_plot, cmap):\n",
    "    for act,Xgb in X.groupby(\"Activity\"):\n",
    "        Xgb_first_values = Xgb.values[:sample_to_plot,:-1]\n",
    "        x = Xgb_first_values[0]\n",
    "        ax.plot(x, linewidth=1, color=cmap(act-1), label = label_dic[act])\n",
    "        for x in Xgb_first_values[1:]:\n",
    "            ax.plot(x, linewidth=1, color=cmap(act-1))\n",
    "def plot_one_axe_shuffle(X, fig, ax, sample_to_plot, cmap):\n",
    "    plot_data = []\n",
    "    for act,Xgb in X.groupby(\"Activity\"):\n",
    "        Xgb_first_values = Xgb.values[:sample_to_plot,:-1]\n",
    "        x = Xgb_first_values[0]\n",
    "        ax.plot(x, linewidth=1, color=cmap(act-1), label = label_dic[act])\n",
    "        for x in Xgb_first_values[1:]:\n",
    "            plot_data.append([x,cmap(act-1)])\n",
    "    random.shuffle(plot_data)\n",
    "    for x,color in plot_data:\n",
    "        ax.plot(x, linewidth=1, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Tracés de tous les signaux\n",
    "Tous les signaux sont tracés par type en superposant les activités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:44.902940Z",
     "start_time": "2020-04-14T09:09:31.611691Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_to_plot = 50\n",
    "index_per_act = [list(zip(np.repeat(act, sample_to_plot), np.where(Y_train_label==act)[0][:sample_to_plot])) for act in range(1,7)]\n",
    "index_to_plot = list(itertools.chain.from_iterable(index_per_act))\n",
    "random.shuffle(index_to_plot)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "for isignal in range(9):\n",
    "    ax = fig.add_subplot(3,3,isignal+1)\n",
    "    for act , i in index_to_plot:\n",
    "        ax.plot(range(128), X_train[i,:,isignal],color=CMAP(act-1), linewidth=1)\n",
    "        ax.set_title(SIGNALS[isignal])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Apprécier la difficulté à distinguer les activités au sein d'un même signal.\n",
    "\n",
    "###  Par signal \n",
    "Le seul signal \"acélération en\" x est tracé en distinguant les activités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:46.526860Z",
     "start_time": "2020-04-14T09:09:44.904853Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_to_plot = 10\n",
    "isignal = 1\n",
    "index_per_act_dict = dict([(act, np.where(Y_train_label==act)[0][:sample_to_plot]) for act in range(1,7)])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,8), num=SIGNALS[isignal])\n",
    "for act , index in index_per_act_dict.items():\n",
    "    ax = fig.add_subplot(3,2,act)\n",
    "    for x in X_train[index]:\n",
    "        ax.plot(range(128), x[:,0],color=CMAP(act-1), linewidth=1)\n",
    "    ax.set_title(ACTIVITY_DIC[act])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Quelles est l'activité qui semble se distinguer facilement des autres? \n",
    "\n",
    "**Q** Observer les signaux d'une activité, par exemple `Walking upstairs`. Qu'est ce qui fait que, pour ces signaux ou courbes, une métrique euclidienne classique ($L_2$) est inopérante? \n",
    "\n",
    "**Q** Corrélativement pourquoi est-il important de décomposer un signal dans le domaine des fréquences?\n",
    "\n",
    "\n",
    "###  [Analyse en composantes principales](http://wikistat.fr/pdf/st-m-explo-acp.pdf)\n",
    "Il est important de se faire une idée précise de la structure des données.  Une analyse en composantes principales est adaptée à cet objectif. \n",
    "#### Remarques\n",
    "   - L'ACP n'est pas réduite sur ces données car cette transformation est sans effet sur la piètre qualité des graphiques.\n",
    "   - L'ACP basée sur une métrique euclidienne usuelle ne fait que confirmer les difficultés précedemment identifiées et l'absence de pouvaoir discriminant des données brutes au sens de cette métrique; cette exploration n'est pas approfondies sur ces données. En revanche un autre [calepin](https://github.com/wikistat/Exploration/blob/master/HumanActivityRecognition/Explo-Python-Har-brutes.ipynb) détaille une analyse factorielle discriminante mais avec la même conclusion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction définie ci-après affiche un nuage de points dans un plan factoriel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:46.540744Z",
     "start_time": "2020-04-14T09:09:46.530594Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_pca(X_R, ytrain, fig, ax, nbc, nbc2, label_dic=ACTIVITY_DIC, cmaps = plt.get_cmap(\"Accent\")\n",
    "):\n",
    "    for i in range(6):\n",
    "        xs = X_R[ytrain==i+1,nbc-1]\n",
    "        ys = X_R[ytrain==i+1, nbc2-1]\n",
    "        label = label_dic[i+1]\n",
    "        color = cmaps(i)\n",
    "        ax.scatter(xs, ys, color=color, alpha=.8, s=10, label=label)\n",
    "        ax.set_xlabel(\"PC%d : %.2f %%\" %(nbc,pca.explained_variance_ratio_[nbc-1]*100), fontsize=15)\n",
    "        ax.set_ylabel(\"PC%d : %.2f %%\" %(nbc2,pca.explained_variance_ratio_[nbc2-1]*100), fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACP sur un type de signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:47.938156Z",
     "start_time": "2020-04-14T09:09:46.548289Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "# Choix du signal\n",
    "isignal = 4\n",
    "signal = SIGNALS[isignal]\n",
    "print(\"ACP Sur signal : \" +signal)\n",
    "X_c = pca.fit_transform(X_train[:,:,isignal])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Représentation des parts de variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:48.644737Z",
     "start_time": "2020-04-14T09:09:47.940412Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.bar(range(10), pca.explained_variance_ratio_[:10]*100, align='center',\n",
    "        color='grey', ecolor='black')\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_ylabel(\"Variance\")\n",
    "ax.set_title(\"\", fontsize=35)\n",
    "ax.set_title(u\"Pourcentage de variance expliquée \\n par les premières composantes\", fontsize=20)\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "box=ax.boxplot(X_c[:,0:10],whis=100)\n",
    "ax.set_title(u\"Distribution des premières composantes\", fontsize=20)\n",
    "\n",
    "fig.suptitle(u\"Résultat ACP sur Signal : \" + signal, fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention**: les diagrammes boîtes sont très perturbés pas les distributions des composantes avec une très forte concentration autour de 0 et énormément de valeurs atypiques. D'où l'utilisation du paramètre `whis=100` pour rallonger les moustaches.\n",
    "\n",
    "**Q** Que sont les graphes ci-dessus. Quelles interprétations ou absence d'interprétation en tirer?\n",
    "\n",
    "Représentation du premier plan factoriel;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:49.039696Z",
     "start_time": "2020-04-14T09:09:48.646489Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10), )\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plot_pca(X_c, Y_train_label,fig ,ax ,1 ,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les autres plans factoriels ne sont guère plus informatifs.\n",
    "#### Sur tous les signaux\n",
    "Tous les signaux sont concaténés à plat en un seul signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:52.091255Z",
     "start_time": "2020-04-14T09:09:49.041222Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "print(\"ACP Sur tous les signaux\")\n",
    "X_c = pca.fit_transform(X_train_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:52.852861Z",
     "start_time": "2020-04-14T09:09:52.099072Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.bar(range(10), pca.explained_variance_ratio_[:10]*100, align='center',\n",
    "        color='grey', ecolor='black')\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_ylabel(\"Variance\")\n",
    "ax.set_title(\"\", fontsize=35)\n",
    "ax.set_title(u\"Pourcentage de variance expliqué \\n des premières composantes\", fontsize=20)\n",
    "\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "box=ax.boxplot(X_c[:,0:10],whis=100)\n",
    "ax.set_title(u\"Distribution des premières composantes\", fontsize=20)\n",
    "\n",
    "fig.suptitle(u\"Résultat ACP\", fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:53.240748Z",
     "start_time": "2020-04-14T09:09:52.854459Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10), )\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plot_pca(X_c, Y_train_label,fig ,ax ,1 ,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Quelle activité semble néanmoins facile à identifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exploration des données métier\n",
    "###  Les données\n",
    "L'[archive de l'UCI]() contient également deux fichiers `train` et `test` des 561 caractéristiques (*features*) ou variables \"métier\" calculées dans les domaines temporels et fréquentiels par transformation des signaux bruts.\n",
    "\n",
    "Voici une liste indicative des variables calculées sur chacun des signaux bruts ou couples de signaux:\n",
    "\n",
    "Name|Signification\n",
    "-|-\n",
    "mean | Mean value\n",
    "std | Standard deviation\n",
    "mad | Median absolute value\n",
    "max | Largest values in array\n",
    "min | Smallest value in array\n",
    "sma | Signal magnitude area\n",
    "energy | Average sum of the squares\n",
    "iqr | Interquartile range\n",
    "entropy | Signal Entropy\n",
    "arCoeff | Autorregresion coefficients\n",
    "correlation | Correlation coefficient\n",
    "maxFreqInd | Largest frequency component\n",
    "meanFreq | Frequency signal weighted average\n",
    "skewness | Frequency signal Skewness\n",
    "kurtosis | Frequency signal Kurtosis\n",
    "energyBand | Energy of a frequency interval\n",
    "angle | Angle between two vectors\n",
    "\n",
    "#### Lecture des données métier\n",
    "Moins volumineuses, elles ont été chargées dans le dépôt simultanément à ce calepin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:54.976847Z",
     "start_time": "2020-04-14T09:09:53.243035Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lecture des données d'apprentissage\n",
    "# Attention, il peut y avoir plusieurs espaces comme séparateur dans le fichier\n",
    "Xtrain=pd.read_csv(\"X_train.txt\",sep='\\s+',header=None)\n",
    "Xtrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:54.985255Z",
     "start_time": "2020-04-14T09:09:54.979066Z"
    }
   },
   "outputs": [],
   "source": [
    "# Variable cible\n",
    "ytrain=pd.read_csv(\"y_train.txt\",sep='\\s+',header=None,names=['y'])\n",
    "# Le type dataFrame est inutile et même gênant pour les la suite\n",
    "ytrain=ytrain[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:55.696175Z",
     "start_time": "2020-04-14T09:09:54.987714Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lecture des données de test\n",
    "Xtest=pd.read_csv(\"X_test.txt\",sep='\\s+',header=None)\n",
    "Xtest.shape\n",
    "ytest=pd.read_csv(\"y_test.txt\",sep='\\s+',header=None,names=['y'])\n",
    "ytest=ytest[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [Analyse en composantes principales](http://wikistat.fr/pdf/st-m-explo-acp.pdf)\n",
    "Fonction graphique pour les plans factoriels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:55.705248Z",
     "start_time": "2020-04-14T09:09:55.699148Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_pca(X_R,fig,ax,nbc,nbc2):\n",
    "    for i in range(6):\n",
    "        xs = X_R[ytrain==i+1,nbc-1]\n",
    "        ys = X_R[ytrain==i+1, nbc2-1]\n",
    "        label = ACTIVITY_DIC [i+1]\n",
    "        color = cmaps(i)\n",
    "        ax.scatter(xs, ys, color=color, alpha=.8, s=1, label=label)\n",
    "        ax.set_xlabel(\"PC%d : %.2f %%\" %(nbc,pca.explained_variance_ratio_[nbc-1]*100), fontsize=10)\n",
    "        ax.set_ylabel(\"PC%d : %.2f %%\" %(nbc2,pca.explained_variance_ratio_[nbc2-1]*100), fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul de la matrice des composantes principales. C'est aussi un changement (transformation) de base; de la base canonique dans la base des vecteurs propres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:56.414735Z",
     "start_time": "2020-04-14T09:09:55.708292Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_c = pca.fit_transform(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valeurs propres ou variances des composantes principales\n",
    "Représentation de la décroissance des valeurs propres, les variances des variables ou composantes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:56.688255Z",
     "start_time": "2020-04-14T09:09:56.418044Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_[0:10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un graphique plus explicite décrit les distribution de ces composantes par des diagrames boîtes; seules les premières sont affichées. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:57.183945Z",
     "start_time": "2020-04-14T09:09:56.691838Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.boxplot(X_c[:,0:10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commenter la décroissance des variances, le choix éventuel d'une dimension ou nombre de composantes à retenir sur les 561.\n",
    "#### Représentation des individus ou \"activités\" en ACP\n",
    "Projection dans les principaux plans factoriels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:59.523613Z",
     "start_time": "2020-04-14T09:09:57.186510Z"
    }
   },
   "outputs": [],
   "source": [
    "cmaps = plt.get_cmap(\"Accent\")\n",
    "fig = plt.figure(figsize= (20,20))\n",
    "count = 0\n",
    "for nbc, nbc2,count in [(1,2,1), (2,3,2), (3,4,3), (1,3,4), (2,4,5), (1,4,7)] :\n",
    "    ax = fig.add_subplot(3,3,count)\n",
    "    plot_pca(X_c, fig,ax,nbc,nbc2)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.8, 0.5), markerscale=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Commenter la séparation des deux types de situation par le premier axe.\n",
    "\n",
    "**Q** Que dire sur la forme des nuages?\n",
    "\n",
    "**Q** Que dire sur la plus ou moins bonne séparation des classes?\n",
    "#### Représentation des variables en ACP\n",
    "Lecture des libellés des variables et constitution d'une liste. Souci de la grande dimension (561), les représentations ne sont guère exploitables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:09:59.542767Z",
     "start_time": "2020-04-14T09:09:59.530303Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('features.txt', 'r') as content_file:\n",
    "    featuresNames = content_file.read()\n",
    "columnsNames = list(map(lambda x : x.split(\" \")[1],featuresNames.split(\"\\n\")[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphe des variables illisible en mettant les libellés en clair. Seule une * est représentée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:03.018314Z",
     "start_time": "2020-04-14T09:09:59.546646Z"
    }
   },
   "outputs": [],
   "source": [
    "# coordonnées des variables\n",
    "coord1=pca.components_[0]*np.sqrt(pca.explained_variance_[0])\n",
    "coord2=pca.components_[1]*np.sqrt(pca.explained_variance_[1])\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "for i, j in zip(coord1,coord2, ):\n",
    "    plt.text(i, j, \"*\")\n",
    "    plt.arrow(0,0,i,j,color='r')\n",
    "plt.axis((-1.2,1.2,-1.2,1.2))\n",
    "# cercle\n",
    "c=plt.Circle((0,0), radius=1, color='b', fill=False)\n",
    "ax.add_patch(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identification des variables participant le plus au premier axe. Ce n'est pas plus clair! Seule la réprésentation des individus apporte finalement des éléments de compréhension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:03.026378Z",
     "start_time": "2020-04-14T09:10:03.020595Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.array(columnsNames)[abs(coord1)>.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [Analyse Factorielle Discriminante (AFD)](http://wikistat.fr/pdf/st-m-explo-afd.pdf)\n",
    "#### Principe\n",
    "L'ACP ne prend pas en compte la présence de la variable qualitative à modéliser contrairement à l'analyse factorielle discriminante (AFD) adaptés à ce contexte \"supervisé\" puisque l'activité est connue sur un échantillon d'apprentissage. L'AFD est une ACP des barycentres des classes munissant l'espace des individus d'une métrique spécifique dite de *Mahalanobis*. Métrique définie par l'inverse de la matrice de covariance intraclase. L'objectif est alors de visualiser les capacités des variables à discriminer les classes.\n",
    "\n",
    "La librairie `scikit-learn` ne propose pas de fonction spécifique d'analyse factorielle discriminante mais les coordonnées des individus dans la base des vecteurs discriminants sont obtenues comme résultats de l'analyse discriminante linéaire décisionnnelle. Cette dernière sera utilisé avec une finalité prédictive dans un deuxième temps (autre calepin). \n",
    "\n",
    "Les résultats de la fonction `LinearDiscriminantAnalysis` de `scikit-learn` sont identiques à ceux de la fonction `lda` de R. Elle eest donc utilisée strictement de la même façon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:04.460102Z",
     "start_time": "2020-04-14T09:10:03.032128Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "method = LinearDiscriminantAnalysis() \n",
    "lda=method.fit(Xtrain,ytrain)\n",
    "X_r2=lda.transform(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que signifie le *warning*? Quel traitement faudrait-t-il mettre en oeuvre pour utiliser une analyse discriminante décisionnelle en modélisation ou apprentissage.\n",
    "\n",
    "#### Représentation des individus en AFD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:06.668576Z",
     "start_time": "2020-04-14T09:10:04.464352Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize= (20,20))\n",
    "count = 0\n",
    "for nbc, nbc2,count in [(1,2,1), (2,3,2), (3,4,3), (1,3,4), (2,4,5), (1,4,7)] :\n",
    "    ax = fig.add_subplot(3,3,count)\n",
    "    plot_pca(X_r2, fig,ax,nbc,nbc2)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.8, 0.5), markerscale=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que dire de la séparation des classes. Sont-elles toutes séparables deux à deux?\n",
    "\n",
    "**Q** Que dire de la forme des nuages notamment dans le premier plan?\n",
    "\n",
    "Comme pour l'ACP, la représentation trop complexe des variables n'apporterait rien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [Classification non supervisée](http://wikistat.fr/pdf/st-m-explo-classif.pdf)\n",
    "Cette section n'est pas utile puisque les classes sont connues néanmoins, une approche générale des l'étude de signaux relatant des activités humaines non identifiées *a priori* nécessiterait cette phase de classificatyion non supervisée ou *clustering*. Cette étape permet simplementici  d'illusrer le comportement d'un alorithmes classification non supervisée classique. La matrice de confusion les classes obtenues avec celles connues permet d'en évaluer les performances. \n",
    "#### $k$*-means*\n",
    "Attention, il est nécessaire de centrer et réduire les variables avant d'exécuter un algorithme de classification non supervisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:11.356260Z",
     "start_time": "2020-04-14T09:10:06.670363Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "tps1 = time.perf_counter()\n",
    "X = StandardScaler().fit_transform(Xtrain)\n",
    "km=KMeans(n_clusters=6)\n",
    "km.fit(Xtrain)\n",
    "tps2 = time.perf_counter()\n",
    "print(\"Temps execution Kmeans :\", (tps2 - tps1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:11.426455Z",
     "start_time": "2020-04-14T09:10:11.360623Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pd.DataFrame(confusion_matrix(ytrain, km.labels_)[1:7,0:6].T, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que dire de l'efficacité d'une approche non supervisée pour catégoriser les activités?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Episode 2</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Prévision de l'activité à partir des variables \"métier\"\n",
    "D'autres méthodes sont successivement testées dans les calepins complétant l'étude: SVM, analyse discriminate décisionnelle, $k$ plus proches voisins, forêts aléatoires, réseaux de neurones... Seule la régression logistique est utilisée dans ce calepin pour illustrer la phase d'apprentissage / modélisation pour la prévision du comportement.\n",
    "\n",
    "###  [Régression logistique](http://wikistat.fr/pdf/st-m-app-rlogit.pdf)\n",
    "\n",
    "####  Principe\n",
    "Une méthode statistique ancienne mais finalement efficace sur ces données. La régression logistique est adaptée à la prévision d'une variable binaire. Dans le cas multiclasse, la fonction logistique de la librairie `Scikit-learn` estime *par défaut* **un modèle par classe**: une classe contre les autres. \n",
    "\n",
    "La probabilité d'appartenance d'un individu à une classe est modélisée à l'aide d'une combinaison linéaire des variables explicatives. Pour transformer une combinaison linéaire à valeur dans $R$ en une probabilité à valeurs dans l'intervalle $[0, 1]$, une fonction de forme sigmoïdale est appliquée.  Ceci donne: $$P(y_i=1)=\\frac{e^{Xb}}{1+e^{Xb}}$$ ou, c'est équivalent, une décomposition linéaire du *logit* ou *log odd ratio* de  $P(y_i=1)$:  $$\\log\\frac{P(y_i=1)}{1-P(y_i=1)}=Xb.$$\n",
    "\n",
    "\n",
    "####  Estimation du modèle sans optimisation\n",
    "Le modèle est estimé sans chercher à raffiner les valeurs de certains paramètres (pénalisation). Ce sera fait dans un deuxième temps. Le paramètre de choix du *solver* est précisé car celui par défaut (`lbfgs`) semble converger moins vite. Une comparaison systématique des différentes options (`liblinear, lbfgs, saga, sag, newton-cg`) seraient bienvenue en association avec le choix de modèle lorsque le nombre de classes est plus grand que 2: fonction perte multinomiale ou un modèle binomial par classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:18.398790Z",
     "start_time": "2020-04-14T09:10:11.430699Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "ts = time.time()\n",
    "method = LogisticRegression(solver='liblinear',multi_class='auto')\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prévision de l'activité de l'échantillon test\n",
    "Une fois le modèle estimé, l'erreur de prévision est évaluée, sans biais optimiste, sur un autre échantillon, dit échantillon test, qui n'a pas participé à l'apprentissage du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:10:18.424156Z",
     "start_time": "2020-04-14T09:10:18.400699Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Quelles sont les classes qui restent difficiles à discriminer?\n",
    "\n",
    "**Q** Commenter la qualité des résultats obtenus. Sont-ils cohérents avec l'approche exploratoire.\n",
    "\n",
    "#### Optimisation du modèle par pénalisation Lasso\n",
    "*Attention* l'exécution est un peu longue... cette optimisation peut être sautée en première lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:19.967342Z",
     "start_time": "2020-04-14T09:10:18.428545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optimisation du paramètre de pénalisation\n",
    "# grille de valeurs\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "ts = time.time()\n",
    "param=[{\"C\":[0.5,1,5,10,12,15,30]}]\n",
    "logit = GridSearchCV(LogisticRegression(penalty=\"l1\",solver='liblinear', \n",
    "                                        multi_class='auto'), param,cv=10,n_jobs=-1)\n",
    "logitOpt=logit.fit(Xtrain, ytrain)  \n",
    "# paramètre optimal\n",
    "logitOpt.best_params_[\"C\"]\n",
    "te = time.time()\n",
    "print(\"Temps : %d secondes\" %(te-ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:19.973912Z",
     "start_time": "2020-04-14T09:19:19.969854Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" % (logitOpt.best_score_,logitOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:19.995153Z",
     "start_time": "2020-04-14T09:19:19.976014Z"
    }
   },
   "outputs": [],
   "source": [
    "yChap = logitOpt.predict(Xtest)\n",
    "# matrice de confusion\n",
    "logitOpt.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:20.021501Z",
     "start_time": "2020-04-14T09:19:19.996900Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(ytest, yChap), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** L'amélioration est-elle bien significative au regard du temps de calcul?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Analyse discriminante linéaire](http://wikistat.fr/pdf/st-m-app-add.pdf)\n",
    "**Q** Que dire de l'optimisation de cette métode? Celle-ci est proposée dans une librairie de R mais pas disponible en python.\n",
    "\n",
    "**Q** L'analyse discriminante quadratique pose quelques soucis. Pourquoi?\n",
    "\n",
    "**Q** Quel paramètre de cet algortithme pourrait être optimisé?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:21.325921Z",
     "start_time": "2020-04-14T09:19:20.024699Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "ts = time.time()\n",
    "method = LinearDiscriminantAnalysis()\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:21.356007Z",
     "start_time": "2020-04-14T09:19:21.327902Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [*K* plus proches voisins](http://wikistat.fr/pdf/st-m-app-add.pdf)\n",
    "\n",
    "Cette méthode peut être vue comme un cas particulier d'analyse discriminante avec une estimation locale des fonctions de densité conditionnelle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:39.487153Z",
     "start_time": "2020-04-14T09:19:21.357706Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "ts = time.time()\n",
    "method = KNeighborsClassifier(n_jobs=-1)\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()\n",
    "t_total = te-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:19:39.508495Z",
     "start_time": "2020-04-14T09:19:39.488760Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SVM linéaire](http://wikistat.fr/pdf/st-m-app-svm.pdf)\n",
    "\n",
    "**Q** Est-il utile d'optimiser le paramètre de pénalisation dans le cas linéaire? Pourquoi?\n",
    "\n",
    "Le nombre max d'itérations a été très sensiblement augmenté (1000 par défaut) sans pour autant améliorer la performance. Le cas multi-classes est traité en considérant une classe contre les autres donc 6 modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:20:08.714818Z",
     "start_time": "2020-04-14T09:19:39.511096Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "ts = time.time()\n",
    "method = LinearSVC(max_iter=20000)\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()\n",
    "t_total = te-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:20:08.740091Z",
     "start_time": "2020-04-14T09:20:08.716703Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [SVM avec noyau gaussien](http://wikistat.fr/pdf/st-m-app-svm.pdf)\n",
    "\n",
    "Apprentissage avec les valeurs par défaut puis optimisation des paramètres.\n",
    "\n",
    "**Q** Quels sont les paramètres à optimiser?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:20:36.973697Z",
     "start_time": "2020-04-14T09:20:08.742383Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "ts = time.time()\n",
    "method = SVC(gamma='auto')\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:20:36.992066Z",
     "start_time": "2020-04-14T09:20:36.975386Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Quelle procédure est exécutée ci-après et dans quel but?\n",
    "\n",
    "*Attention*: l'exécution est un peu longue et peut être sautée en preière lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:24:44.746208Z",
     "start_time": "2020-04-14T09:20:36.993400Z"
    }
   },
   "outputs": [],
   "source": [
    "ts = time.time()\n",
    "param=[{\"C\":[4,5,6],\"gamma\":[.01,.02,.03]}]\n",
    "svm= GridSearchCV(SVC(),param,cv=10,n_jobs=-1)\n",
    "svmOpt=svm.fit(Xtrain, ytrain)\n",
    "te = time.time()\n",
    "te-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:24:44.761547Z",
     "start_time": "2020-04-14T09:24:44.756680Z"
    }
   },
   "outputs": [],
   "source": [
    "# paramètre optimal\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" % (svmOpt.best_score_,svmOpt.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comparer les deux approches par SVM (linéaire et radiale): temps de calcul et performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [*Random forest*](http://wikistat.fr/pdf/st-m-app-agreg.pdf)\n",
    "\n",
    "**Q** Quel serait le paramètre à optimiser?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:24:58.273566Z",
     "start_time": "2020-04-14T09:24:44.764426Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "ts = time.time()\n",
    "method = RandomForestClassifier(n_estimators=200,n_jobs=-1)\n",
    "method.fit(Xtrain,ytrain)\n",
    "score = method.score(Xtest, ytest)\n",
    "ypred = method.predict(Xtest)\n",
    "te = time.time()\n",
    "t_total = te-ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:24:58.291331Z",
     "start_time": "2020-04-14T09:24:58.275291Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Score : %f, time running : %d secondes\" %(score, te-ts))\n",
    "pd.DataFrame(confusion_matrix(ytest, ypred), index = labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q** Les méthodes non linéaires sophistiquées sont elles pertinentes sur ces données?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinaison de modèles\n",
    "Les formes des nuages de chaque classe observées dans le premier plan de l'analyse en composantes principales montrent que la structure de covariance n'est pas identique dans chaque classe. Cette remarque suggèrerait de s'intéresser à l'analyse discriminante quadratique mais celle-ci bloque sur l'estimation des six matrices de covariance et de leur inverse. Néanmoins il semble que, plus précisément, deux groupes se distinguent: les classes actives (marcher, monter ou descendre un escalier) d'une part et les classes passives (couché, assis, debout) d'autre part et, qu'à l'intérieur de chaque groupe les variances sont assez similaires. \n",
    "\n",
    "Cette situation suggère de construire une décision en deux étapes ou hiérarchique:\n",
    "1. Régression logistique séparant les activités passives *vs.* actives,\n",
    "2. Un modèle spécifique à chacune des classes précédentes, par exemple des SVM à noyau gaussien.\n",
    "\n",
    "Une telle construction hiérarchique de modèles aboutit à une précision supérieure à 97%.\n",
    "\n",
    "**Exo** Programmer une telle approche. en utilisant les capacités de python pour réaliser un *pipeline* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Episode 3</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Prévision de l'activité à partir des signaux bruts\n",
    "###  Introduction\n",
    "Comme expliqué en introduction, le calcul des nombreuses transformations des données est bien trop consommateur des ressources de la batterie d'un objet connecté. Cette section se propose d'utiliser les seules signaux bruts pour faire apprendre un algorithme et parmi ceux-ci seuls les réseaux de neurones pouvant être \"cablés\" dans un circuit sont pris en compte. En effet un algorithme de type XGBoost (extrem gradient boosting) parvient également à de bons résultats sur les signaux mais à un coût trop élevé. \n",
    "\n",
    "Deux algorithmes sont successivement testés: un réseau de type perceptron classique suivi d'un réseau avec couche de convolution 1D sur les signaux. \n",
    "\n",
    "Il faudrait ajouter que de nombreuses configurations on été testées, merci aux étudiants de l'INSA de Toulouse de la spécialité Mathématiques Appliquées - Science des Données: LSTM, convolution 2D... avant d'adopter celle proposée ci-dessous. C'est une réalité, de l'apprentissage profond, sans fondement théorique précis, seule une approche très heuristique des innombrables solutions d'empilements possibles de couches, permet de déterminer une configuration  plus performante. D'autres structures de réseau seraient à tester pour atteindre les 96% de bien classés de la solution précédente. Une autre amélioration possible consisterait à enrichir la base de données par des simulations de données synthétiques, par exemple des translations aléatoires des signaux observés; fonction intégrée à la librairie `Keras` mais qui alourdit les temps d'apprentissage.\n",
    "\n",
    "###  Perceptron à une couche cachée\n",
    "#### Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:28:55.058629Z",
     "start_time": "2020-04-14T09:28:52.862017Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.models as km \n",
    "import tensorflow.keras.layers as kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:28:55.068108Z",
     "start_time": "2020-04-14T09:28:55.062816Z"
    }
   },
   "outputs": [],
   "source": [
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}\n",
    "def my_confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Définition du réseau \n",
    "Une couche cachée, une couche de reformattage puis une couche de sortie à 6 classes. Le nombre de neurones (50) sur la couche cachée a été optimisé par ailleurs. Le nombre d'epochs reste raisonnable et la taille des batchs devraient être optimisés surtout dans le cas d'utilisation d'une carte GPU.\n",
    "\n",
    "Remarquer le nombre de paramètres à estimer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:28:55.273143Z",
     "start_time": "2020-04-14T09:28:55.069755Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs=20\n",
    "batch_size=32\n",
    "n_hidden = 50\n",
    "\n",
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = 6\n",
    "\n",
    "model_base_mlp =km.Sequential()\n",
    "model_base_mlp.add(kl.Dense(n_hidden, input_shape=(timesteps, input_dim),  activation = \"relu\"))\n",
    "model_base_mlp.add(kl.Reshape((timesteps*n_hidden,) , input_shape= (timesteps, n_hidden)  ))\n",
    "model_base_mlp.add(kl.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model_base_mlp.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model_base_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:30:14.297438Z",
     "start_time": "2020-04-14T09:28:55.275469Z"
    }
   },
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "model_base_mlp.fit(X_train,  Y_train_dummies, batch_size=batch_size, validation_data=(X_test, Y_test_dummies), epochs=epochs)\n",
    "t_end = time.time()\n",
    "t_learning = t_end-t_start\n",
    "\n",
    "score = model_base_mlp.evaluate(X_test, Y_test_dummies)[1] \n",
    "print(\"\\nScore With Simple MLP on Multidimensional Inertial Signals = %.2f, Learning time = %.2f secondes\" %(score*100, t_learning) )\n",
    "metadata_mlp = {\"time_learning\" : t_learning, \"score\" : score}\n",
    "base_mlp_prediction = model_base_mlp.predict(X_test)\n",
    "\n",
    "my_confusion_matrix(Y_test_dummies, base_mlp_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Réseau avec couche convolutionnelle\n",
    "L'idée pertinente avec ces données est évidemment d'identifier le problème lié au déphasage des signaux. L'utilisation d'une couche convolutionnelle introduit une propriété d'invariance par translation. Les caractéristiques ou *features* sortant de cette couche acquièrent donc ainsi de bonnes propriétés avant d'être dirigées vers des couches techniques intermédiaires (`MaxPooling, Flatten`) et une dernière couche de sortie qui effectue la discrimination à partir des caractéristiques.\n",
    "\n",
    "**Q** Remarquer le nombre de paramètres à estimer, le comparer avec celui du perceptron précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:30:14.487975Z",
     "start_time": "2020-04-14T09:30:14.304720Z"
    }
   },
   "outputs": [],
   "source": [
    "timesteps = len(X_train[0])\n",
    "batch_size= 32\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = 6\n",
    "epochs=20\n",
    "\n",
    "model_base_conv_1D =km.Sequential()\n",
    "model_base_conv_1D.add(kl.Conv1D(32, 9, activation='relu', input_shape=(timesteps, input_dim)))\n",
    "model_base_conv_1D.add(kl.MaxPooling1D(pool_size=3))\n",
    "model_base_conv_1D.add(kl.Flatten())\n",
    "model_base_conv_1D.add(kl.Dense(n_classes, activation='softmax'))\n",
    "model_base_conv_1D.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model_base_conv_1D.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:31:51.302103Z",
     "start_time": "2020-04-14T09:30:14.491425Z"
    }
   },
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "model_base_conv_1D.fit(X_train,  Y_train_dummies, batch_size=batch_size, validation_data=(X_test, Y_test_dummies), epochs=epochs)\n",
    "t_end = time.time()\n",
    "t_learning = t_end-t_start\n",
    "\n",
    "score = model_base_conv_1D.evaluate(X_test, Y_test_dummies)[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:31:52.373738Z",
     "start_time": "2020-04-14T09:31:51.309760Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata_conv = {\"time_learning\" : t_learning, \"score\" : score}\n",
    "base_conv_1D_prediction = model_base_conv_1D.predict(X_test)\n",
    "my_confusion_matrix(Y_test_dummies, base_conv_1D_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T09:31:52.391143Z",
     "start_time": "2020-04-14T09:31:52.377129Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Score With Conv on Multidimensional Inertial Signals = %.2f, Learning time = %.2f secondes\" %(score*100, t_learning) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques efforts supplémentaires permettraient sans doute de gratter quelques points sur la précision du résultat mais... attention au sur-apprentissage si le même échantillon test est toujours utilisé. Le volume des données est sans doute insuffisant pour atteindre cet objectif ou alors il faudrait tester une \"augmentation des données\"  en opérant des translations des signaux.\n",
    "\n",
    "L'objectif final de ce calepin n'est pas de trouver la meilleure solution mais bien de montrer qu'un traitement des signaux bruts avec les algorithmes adaptés conduit finalement à des résultats aussi pertinents que le traitement des données issues de l'expertise de spécialistes du traitement du signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <FONT COLOR=\"Red\">Episode 4</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  [Détection d'anomalies](http://wikistat.fr/pdf/st-m-app-anomalies.pdf)\n",
    "###  Introduction\n",
    "Comme expliqué dans l'introduciton générale, l'objectif est de détecter des situations qui ne seraient pas représentées dans la base d'apprentissage et donc très probablement sources d'erreur de prévision. Ces méthodes visent également à détecter des défaillances dans des appareils de mesure ou encore des dérives dans un processus de production. La détection d'anomalie est aussi l'étape préalable à celle de détection d'une défaillance probable et donc de la mise en place et l'industrialisation d'un processus de maintenance prédictive.\n",
    "\n",
    "Ces données ont été choisies car de type signal ou fonctionnel, elles sont représentatives de très nombreux domaines d'application: industrie, santé, observation de la terre... Pour aborder ce type de données, le choix d'une base de réprésentation est fondamental, il dépend des caractéristiques des données et de l'objectif visé. Différentes bases sont souvent à tester et ont été utilisées dans ce calepin: \n",
    "- base de fonctions propres issues d'une SVD (transformation de Karhunen Loeve ou analyse en composantes principales);\n",
    "- base de fonctions splines (lissage) pour éliminer un bruit de mesure parasite sur un signal supposé régulier (continu voire dérivable);\n",
    "- base d'ondelettes pour débruiter un signal présentant des singularités dont la localisation est porteuse de sens;\n",
    "- base de Fourier pour des signaux stationnaires voire périodiques comme les données de ce calepin.\n",
    "\n",
    "Comme précédemment, deux stratégies sont testées: détection d'anomalies sur les données \"métier\" puis sur des transformations élémentaires des données brutes. Plusieurs méthodes sont testées et comparées sur ces deux types de données:\n",
    "- CAH : Classification ascendante hiérarchique\n",
    "- One-class SVM\n",
    "- Local Outlier Factor\n",
    "- Isolation Forest\n",
    "\n",
    "***NB***  Une troisième stratégie basée sur les seuls signaux bruts nécessiteraient le probable déploiement d'un type particulier de réseau de neurones sous la forme d'un *auto-encodeur variationnel*. Cf. par exemple [Jabar et al. 2019](https://link.springer.com/chapter/10.1007/978-3-030-37599-7_59). \n",
    "\n",
    "###  Les données\n",
    "Afin de pourvoir excuter cette partie indépendamment des précédentes, les librairies sont rechargées et les données relues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# ACP\n",
    "import sklearn.decomposition as sd\n",
    "import sklearn.preprocessing as sp\n",
    "# Hierarchical clustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "# LOF\n",
    "import sklearn.neighbors as sn\n",
    "# Isolation Forest\n",
    "import sklearn.ensemble as se\n",
    "# Plot et Display\n",
    "import utils.illustration as uil\n",
    "import utils.load as ul\n",
    "\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "sb.set()\n",
    "sb.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGNALS = [ \"body_acc_x\", \"body_acc_y\", \"body_acc_z\", \"body_gyro_x\", \"body_gyro_y\", \"body_gyro_z\"]\n",
    "CMAP = plt.get_cmap(\"Set1\")\n",
    "ACTIVITY_DIC = {1 : \"WALKING\",\n",
    "2 : \"WALKING UPSTAIRS\",\n",
    "3 : \"WALKING DOWNSTAIRS\",\n",
    "4 : \"SITTING\",\n",
    "5 : \"STANDING\",\n",
    "6 : \"LAYING\"}\n",
    "COLOR_DIC = {v:CMAP(k-2) if v!=\"WALKING\" else CMAP(10) for k,v in ACTIVITY_DIC.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./UCI HAR Dataset\"\n",
    "#Multidimensional Data\n",
    "X_train = ul.load_signals(data_path,\"train\", SIGNALS)\n",
    "Y_train_label = ul.load_y(data_path, \"train\")\n",
    "X_train_metier= ul.my_read_csv(data_path+\"/train/X_train.txt\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constitution of the datasets with anomalies\n",
    "We build a dataset with\n",
    "\n",
    "   * `N_normal` signals considered as *normal*  (corresponding to the class *WALKING*).  \n",
    "   * `N_anormal` signals of each type of *abnormal* signals  (*WALKING UPSTAIRS*, *WALKING DOWNSTAIRS*, *SITTING*, *STANDING*, *LAYING*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_normal = 800\n",
    "N_anormal = 2\n",
    "\n",
    "# New Y Label\n",
    "Y= np.hstack([np.repeat(1,N_normal)] + [np.repeat(i, N_anormal) for i in range(2,7)])\n",
    "Y_label = np.array([ACTIVITY_DIC[y] for y in Y])\n",
    "#New X Data\n",
    "index_per_act = np.hstack([np.where(Y_train_label==1)[0][:N_normal]] + [np.where(Y_train_label==act)[0][:N_anormal] for act in range(2,7)])\n",
    "\n",
    "X = X_train[index_per_act]\n",
    "X_metier = X_train_metier[index_per_act]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each type of signal, a sample of the normal behavior and the different anomalies are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_sample_per_activity = dict([(v,50) if v==\"WALKING\" else (v,N_anormal) for k,v in ACTIVITY_DIC.items()])\n",
    "linestyle_per_activity = dict([(v,\"dashed\") if v==\"WALKING\" else (v,\"solid\") for k,v in ACTIVITY_DIC.items()])\n",
    "linewidth_per_activity = dict([(v,1) if v==\"WALKING\" else (v,2) for k,v in ACTIVITY_DIC.items()])\n",
    "\n",
    "fig = plt.figure(figsize=(16,18))    \n",
    "uil.plot_signaux(fig, X, Y_label, SIGNALS, COLOR_DIC, nb_sample_per_activity, \n",
    "             linestyle_per_activity, linewidth_per_activity, figdim1 = 3, figdim2 = 2, legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal components analysis\n",
    "\n",
    "#### For the signal : acceleration in  x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isignal = 0\n",
    "print(\"ACP on signal \" + SIGNALS[isignal])\n",
    "X_signal = np.vstack([x[:,isignal] for x in X])\n",
    "\n",
    "acp = sd.PCA()\n",
    "X_acp_signal = acp.fit_transform(sp.scale(X_signal))\n",
    "\n",
    "X_signal.shape\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "uil.plot_variance_acp(fig, acp, X_acp_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.shape[0]\n",
    "colors=[COLOR_DIC[y] for y in Y_label]\n",
    "markersizes = [60 if y==1 else 140 for y in Y]\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "uil.plot_projection_acp(fig, X_acp_signal, acp, colors=colors, markersizes = markersizes, color_dic=COLOR_DIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comment these results from the perspective of anomaly detection\n",
    "\n",
    "\n",
    "####  For all the signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_signaux = np.vstack([x.reshape(128*6) for x in X])\n",
    "acp = sd.PCA()\n",
    "X_acp_signaux = acp.fit_transform(sp.scale(X_signaux))\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "uil.plot_variance_acp(fig, acp, X_acp_signaux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "uil.plot_projection_acp(fig, X_acp_signaux, acp, colors, markersizes, color_dic=COLOR_DIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the *features* data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acp = sd.PCA()\n",
    "X_acp_metier = acp.fit_transform(sp.scale(X_metier))\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "uil.plot_variance_acp(fig, acp, X_acp_metier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "uil.plot_projection_acp(fig, X_acp_metier, acp, colors, markersizes, color_dic=COLOR_DIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comment these results from the perspective of anomaly detection\n",
    "\n",
    "###  Anomaly detection of the *features* data \n",
    "\n",
    "It seems quite easy to detect the anomalies from the *features* data. We apply the classical methods studied during the course: hierarchical clustering with the \"single\" option,  One class SVM, Local Outlier Factor and Isolation Forest. \n",
    "The different methods have not been optimized. Study the impact of the different parameters on the anomaly detection performances. \n",
    "\n",
    "\n",
    "#### Hierarchical clustering (Classification Ascendante Hiérarchique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = sch.linkage(X_metier, 'single')\n",
    "C = np.array([c[0] for c in sch.cut_tree(Z,6)])\n",
    "\n",
    "CT_HCA = pd.DataFrame(list(zip(C,Y_label)), columns=[\"pred\",\"Anomaly\"])\n",
    "display(pd.crosstab(CT_HCA.pred, CT_HCA.Anomaly))\n",
    "\n",
    "LABELS = [\"\" if y==\"WALKING\" else y for y in Y_label]\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "sch.dendrogram( Z, p=6, leaf_rotation=45.,leaf_font_size=15,labels=LABELS, truncate_mode=\"level\"  # font size for the x axis labels\n",
    ")\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "\n",
    "ax =fig.get_axes()[0]\n",
    "xlbls = ax.get_xmajorticklabels()\n",
    "for lbl in xlbls:\n",
    "    if lbl.get_text() in COLOR_DIC:\n",
    "        lbl.set_color(COLOR_DIC[lbl.get_text()])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  One class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm as ssvm\n",
    "OCS = ssvm.OneClassSVM(kernel=\"rbf\", nu=0.05)\n",
    "\n",
    "OCS.fit(X_metier)\n",
    "pred = OCS.predict(X_metier)\n",
    "\n",
    "CT_svm = pd.DataFrame(list(zip(pred,Y_label)), columns=[\"pred\",\"Anomaly\"])\n",
    "display(pd.crosstab(CT_svm.pred, CT_svm.Anomaly))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(30,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "uil.plot_detection_result(fig, ax, CT_svm,COLOR_DIC, normal_behaviour=\"WALKING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contamination=0.05\n",
    "metric = \"euclidean\"\n",
    "n_neighbors = 15\n",
    "clf = sn.LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination, metric=metric)\n",
    "y_pred = clf.fit_predict(X_metier)\n",
    "\n",
    "CT_metier_lof = pd.DataFrame(list(zip(y_pred,Y_label)), columns=[\"pred\",\"Anomaly\"])\n",
    "display(pd.crosstab(CT_metier_lof.pred, CT_metier_lof.Anomaly))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "uil.plot_detection_result(fig, ax, CT_metier_lof, COLOR_DIC, normal_behaviour=\"WALKING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = se.IsolationForest(n_estimators=100, contamination=0.05, bootstrap=True, n_jobs=-1)\n",
    "\n",
    "clf.fit(X_metier)\n",
    "y_pred = clf.predict(X_metier)\n",
    "\n",
    "CT_IF = pd.DataFrame(list(zip(y_pred,Y_label)), columns=[\"pred\",\"Anomaly\"])\n",
    "display(pd.crosstab(CT_IF.pred, CT_IF.Anomaly))\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "uil.plot_detection_result(fig, ax, CT_IF, COLOR_DIC, normal_behaviour=\"WALKING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** What is your conclusion on the *features* data ? \n",
    "\n",
    "The objective of the next sections is to try to detect the  anomalies from the raw data or from simple transformations of the raw data. \n",
    "\n",
    "\n",
    "###  Anomaly detection on the raw data\n",
    "\n",
    "#### Hierarchical clustering\n",
    "\n",
    "We first use a single signal : acceleration in x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = sch.linkage(X_signal,'single')\n",
    "\n",
    "C = np.array([c[0] for c in sch.cut_tree(Z,6)])\n",
    "\n",
    "CT_HCA = pd.DataFrame(list(zip(C,Y_label)), columns=[\"pred\",\"Anomaly\"])\n",
    "display(pd.crosstab(CT_HCA.pred, CT_HCA.Anomaly))\n",
    "\n",
    "LABELS = [\"\" if y==\"WALKING\" else y for y in Y_label]\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "sch.dendrogram( Z, p=6, leaf_rotation=45.,leaf_font_size=15,labels=LABELS, truncate_mode=\"level\"  # font size for the x axis labels\n",
    ")\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "\n",
    "ax =fig.get_axes()[0]\n",
    "xlbls = ax.get_xmajorticklabels()\n",
    "for lbl in xlbls:\n",
    "    if lbl.get_text() in COLOR_DIC:\n",
    "        lbl.set_color(COLOR_DIC[lbl.get_text()])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** How many anomalies have been detected? Do you get better results by taking all the signals?\n",
    "\n",
    "#### One class SVM\n",
    "\n",
    "##### On the two first PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm as ssvm\n",
    "OCS = ssvm.OneClassSVM(kernel=\"rbf\", nu=0.01)\n",
    "\n",
    "OCS.fit(X_acp_signal[:,:2])\n",
    "pred = OCS.predict(X_acp_signal[:,:2])\n",
    "\n",
    "CT_svm = pd.DataFrame(list(zip(pred,Y_label)), columns=[\"pred\",\"Anomaly\"])\n",
    "display(pd.crosstab(CT_svm.pred, CT_svm.Anomaly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comment on the results. Do you get better results by increasing the number of components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_acp = X_acp_signal\n",
    "nu = 0.02\n",
    "# fit the model\n",
    "clf = ssvm.OneClassSVM(kernel=\"rbf\",nu=nu)\n",
    "clf.fit(X_acp[:,:2])\n",
    "y_pred_train = clf.predict(X_acp[:,:2])\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "markersizes = [10 if y==1 else 20 for y in Y]\n",
    "labels = [\"\"] * N \n",
    "for il, l in [(np.where(Y_label==y)[0][0],y) for y in set(Y_label)]:\n",
    "    labels[il] = l\n",
    "\n",
    "uil.plot_decision_function(fig, ax, clf, X_acp, y_pred_train, colors=colors, labels = labels, markersizes=markersizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.set_title(\"Novelty Detection : nu=%.1f\" %nu)\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "uil.plot_detection_result(fig, ax, CT_svm, COLOR_DIC, normal_behaviour=\"WALKING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On the acceleration in x :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm as ssvm\n",
    "OCS = ssvm.OneClassSVM(kernel=\"rbf\", nu=0.05)\n",
    "OCS.fit(X_signal)\n",
    "pred = OCS.predict(X_signal)\n",
    "CT_svm = pd.DataFrame(list(zip(pred,Y_label)), columns=[\"pred\",\"Anomaly\"])\n",
    "display(pd.crosstab(CT_svm.pred, CT_svm.Anomaly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comment on the results. Does applying the method on all the signals improve the results?\n",
    "\n",
    "#### Local Outlier Factor\n",
    "\n",
    "##### On the acceleration in  x : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contamination=0.05\n",
    "metric = \"euclidean\"\n",
    "n_neighbors = 15\n",
    "clf = sn.LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination, metric=metric)\n",
    "y_pred = clf.fit_predict(X_signal)\n",
    "\n",
    "CT_lof = pd.DataFrame(list(zip(y_pred,Y_label)), columns=[\"pred\",\"Anomaly\"])\n",
    "display(pd.crosstab(CT_lof.pred, CT_lof.Anomaly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "uil.plot_detection_result(fig, ax, CT_lof, COLOR_DIC, normal_behaviour=\"WALKING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contamination=0.05\n",
    "metric = \"euclidean\"\n",
    "n_neighbors = 15\n",
    "clf = sn.LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination, metric=metric)\n",
    "y_pred = clf.fit_predict(X_signaux)\n",
    "CT_tous_lof = pd.DataFrame(list(zip(y_pred,Y_label)), columns=[\"pred\",\"Anomaly\"])\n",
    "display(pd.crosstab(CT_tous_lof.pred, CT_tous_lof.Anomaly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comment on the results. Do you get better performances on all the signals? By changing the parameters?\n",
    "\n",
    "##### On the PCA components : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_acp = X_acp_signal\n",
    "n_neighbors = 15\n",
    "\n",
    "# fit the model\n",
    "clf = sn.LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination, metric = metric)\n",
    "y_pred = clf.fit_predict(X_acp[:,:2])\n",
    "\n",
    "CT_ACP_lof = pd.DataFrame(list(zip(y_pred,Y_label)), columns=[\"pred\",\"Anomaly\"])\n",
    "display(pd.crosstab(CT_lof.pred, CT_ACP_lof.Anomaly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "uil.plot_detection_result(fig, ax, CT_ACP_lof, COLOR_DIC, normal_behaviour=\"WALKING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Souci*: les anomalies détectées ne s'affichent pas !\n",
    "\n",
    "\n",
    "####  Isolation Forest\n",
    "\n",
    "##### On the acceleration in  x :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = se.IsolationForest(n_estimators=100, contamination=0.05, bootstrap=True, n_jobs=-1)\n",
    "\n",
    "\n",
    "clf.fit(X_signal)\n",
    "y_pred = clf.predict(X_signal)\n",
    "\n",
    "CT_IF = pd.DataFrame(list(zip(y_pred,Y_label)), columns=[\"pred\",\"Anomaly\"])\n",
    "display(pd.crosstab(CT_IF.pred, CT_IF.Anomaly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "uil.plot_detection_result(fig, ax, CT_IF, COLOR_DIC, normal_behaviour=\"WALKING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On the PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_acp = X_acp_signal\n",
    "\n",
    "contamination=0.05\n",
    "clf = se.IsolationForest(n_estimators=100, contamination=contamination, bootstrap=True, n_jobs=-1)\n",
    "clf.fit(X_acp[:,:2])\n",
    "y_pred = clf.predict(X_acp[:,:2])\n",
    "\n",
    "CT_IF = pd.DataFrame(list(zip(y_pred,Y_label)), columns=[\"pred\",\"Anomaly\"])\n",
    "display(pd.crosstab(CT_IF.pred, CT_IF.Anomaly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Do you get better results by increasing the number of PCA components ? \n",
    "\n",
    "**In conclusion**, the anomaly detection methods applied directly to the signals do not work well. We will see in the next section if the projection onto a wavelet basis allows to obtain better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Anomaly detection with the FFT coefficients\n",
    "#### Computation of the FFT coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT Coefficients : \n",
    "\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "isignal = 0\n",
    "print(\" signal \" + SIGNALS[isignal])\n",
    "X_signal = np.vstack([x[:,isignal] for x in X])\n",
    "\n",
    "#print(amplitudefft)\n",
    "#plt.plot(amplitudefft)\n",
    "\n",
    "fftCoeff = []\n",
    "\n",
    "for x in X_signal :\n",
    "    \n",
    "    mx=np.mean(x)\n",
    "    x_centre=x-mx\n",
    "   #Apply fast Fourier transform\n",
    "    coeffsfft=np.abs(fft(x_centre))  \n",
    "    coeffsfft_flatten = np.hstack(coeffsfft)\n",
    "    fftCoeff.append(coeffsfft_flatten)\n",
    "        \n",
    "fftCoeff = np.array(fftCoeff)\n",
    "\n",
    "# Just keep half of the coefficients (they are then repeated symmetrically)\n",
    "\n",
    "fftCoeff=fftCoeff[:,:64]\n",
    "print(fftCoeff.shape)\n",
    "print(np.sum(fftCoeff!=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA of the FFT coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acp = sd.PCA()\n",
    "X_acp_fft = acp.fit_transform(sp.scale(fftCoeff))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "uil.plot_variance_acp(fig, acp, X_acp_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "uil.plot_projection_acp(fig, X_acp_fft, acp, colors, markersizes, color_dic=COLOR_DIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Which signals differ from the others? Is this consistent with what has been seen in the previous notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = sch.linkage(fftCoeff, 'single')\n",
    "C = np.array([c[0] for c in sch.cut_tree(Z,6)])\n",
    "\n",
    "CT_HCA = pd.DataFrame(list(zip(C,Y_label)), columns=[\"pred\",\"Anomaly\"])\n",
    "display(pd.crosstab(CT_HCA.pred, CT_HCA.Anomaly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\"\" if y==\"WALKING\" else y for y in Y_label]\n",
    "fig = plt.figure(figsize=(25, 10))\n",
    "sch.dendrogram( Z, p=6, leaf_rotation=45.,leaf_font_size=15,labels=LABELS, truncate_mode=\"level\"  # font size for the x axis labels\n",
    ")\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "\n",
    "ax =fig.get_axes()[0]\n",
    "xlbls = ax.get_xmajorticklabels()\n",
    "for lbl in xlbls:\n",
    "    if lbl.get_text() in COLOR_DIC:\n",
    "        lbl.set_color(COLOR_DIC[lbl.get_text()])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comment the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm as ssvm\n",
    "OCS = ssvm.OneClassSVM(kernel=\"rbf\", nu=0.05)\n",
    "\n",
    "OCS.fit(fftCoeff)\n",
    "pred = OCS.predict(fftCoeff)\n",
    "\n",
    "CT_FFT_svm = pd.DataFrame(list(zip(pred,Y_label)), columns=[\"pred\",\"Anomaly\"])\n",
    "display(pd.crosstab(CT_FFT_svm.pred, CT_FFT_svm.Anomaly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Study the impact of the kernel and of the parameter `nu`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contamination=0.05\n",
    "metric = \"euclidean\"\n",
    "n_neighbors = 15\n",
    "clf = sn.LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination, metric=metric)\n",
    "y_pred = clf.fit_predict(fftCoeff)\n",
    "\n",
    "CT_FFT_lof = pd.DataFrame(list(zip(y_pred,Y_label)), columns=[\"pred\",\"Anomaly\"])\n",
    "display(pd.crosstab(CT_FFT_lof.pred, CT_FFT_lof.Anomaly))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "uil.plot_detection_result(fig, ax, CT_FFT_lof, COLOR_DIC, normal_behaviour=\"WALKING\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Comment on the results and see the impact of the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = se.IsolationForest(n_estimators=100, contamination=0.05, bootstrap=True, n_jobs=-1)\n",
    "\n",
    "clf.fit(fftCoeff)\n",
    "y_pred = clf.predict(fftCoeff)\n",
    "\n",
    "CT_IF = pd.DataFrame(list(zip(y_pred,Y_label)), columns=[\"pred\",\"Anomaly\"])\n",
    "display(pd.crosstab(CT_IF.pred, CT_IF.Anomaly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Visualisation of the  LOF results\n",
    "\n",
    "The LOF method is one of the most powerful whatever the type of \"features\" considered. The results of this method are visualized for the various cases considered in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(40,40))\n",
    "ax = fig.add_subplot(3,2,1)\n",
    "uil.plot_detection_result(fig, ax, CT_lof, COLOR_DIC, normal_behaviour=\"WALKING\")\n",
    "ax.set_title(\"Raw data- Boddy acc x \", fontsize=25)\n",
    "ax = fig.add_subplot(3,2,2)\n",
    "uil.plot_detection_result(fig, ax, CT_tous_lof, COLOR_DIC, normal_behaviour=\"WALKING\")\n",
    "ax.set_title(\"Raw data- All the signals\", fontsize=25)\n",
    "ax = fig.add_subplot(3,2,3)\n",
    "uil.plot_detection_result(fig, ax, CT_ACP_lof, COLOR_DIC, normal_behaviour=\"WALKING\")\n",
    "ax.set_title(\"PCA, 2 first components\", fontsize=25)\n",
    "#ax = fig.add_subplot(3,2,4)\n",
    "#uil.plot_detection_result(fig, ax, CT_ond_lof, COLOR_DIC, normal_behaviour=\"WALKING\")\n",
    "#ax.set_title(\"Wavelet coefficients, levels 1 to 4 \", fontsize=25)\n",
    "ax = fig.add_subplot(3,2,5)\n",
    "uil.plot_detection_result(fig, ax, CT_metier_lof, COLOR_DIC, normal_behaviour=\"WALKING\")\n",
    "ax.set_title(\"Features Data\", fontsize=25)\n",
    "ax = fig.add_subplot(3,2,6)\n",
    "uil.plot_detection_result(fig, ax, CT_FFT_lof, COLOR_DIC, normal_behaviour=\"WALKING\")\n",
    "ax.set_title(\"FFT coefficients\", fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have studied various methods for anomaly detection. On the *features* data, it is quite easy  to detect the anomalies. On the functional raw data, we have seen the importance of defining good \"features\" to highlight the anomalies: the methods of detection of anomalies applied to the raw signals or their wavelet transform, have certainly not been totally optimized but do not give good results in this case. On the other hand, the Fast Fourier Transform highlights the anomalies for these data. We can not draw a generality : on simulated telemetry data of the available notebook [here](https://github.com/wikistat/High-Dimensional-Deep-Learning/blob/master/AnomalyDetection/Python-Anomaly-Detection.ipynb), the transformation onto a wavelet basis is relevant for the detection of anomalies for these functional data. It is therefore important to know the data and the type of anomalies that you want to detect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
