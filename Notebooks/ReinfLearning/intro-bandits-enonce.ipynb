{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to bandit problems and algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import *\n",
    "from math import *\n",
    "from numpy import *\n",
    "from numpy.random import *\n",
    "from scipy.misc import *\n",
    "from scipy.stats import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sequel we consider the stochastic bandit problem with only two actions $0$ and $1$. We assume that their rewards are Bernoulli random variables with parameters $\\mu_0 = 0.6$ and $\\mu_1 = 0.4$ (so that action $0$ is optimal, but we have to learn it from experience!).\n",
    "\n",
    "### Simulation of Bernoulli random variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial(n=10,p=0.5,size=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomial(n=1,p=0.5,size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation:\n",
    "- The arm played by the algorithm at time $t$ is denoted by $I_t$.\n",
    "- The reward of arm $a$ at time $t$ is $g_t(a)$.\n",
    "- We write $\\hat{\\mu}_t(a)$ for the average of the rewards obtained from arm $a \\in \\{0,1\\}$ between iterations $1$ and $t$.\n",
    "- We also write $T_a(t)$ for the number of times arm $a$ was played up to iteration $t$.\n",
    "- In particular, we have\n",
    "$$\\hat{\\mu}_t(a) = \\frac{1}{T_a(t)} \\sum_{k=1}^t g_k(a) \\mathbb{1}_{I_k = a}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why stopping exploring is a bad idea if we do not know the time horizon.\n",
    "\n",
    "The \"Explore-Then-Commit\" algorithm proceeds as follows:\n",
    "- alternatively play actions $0$ and $1$ from iteration $t=1$ up to iteration $t=\\tau$\n",
    "- determine the empirically best arm\n",
    "$$A \\in \\rm{argmax}_{a \\in \\{0,1\\}} \\hat{\\mu}_{\\tau}(a)$$\n",
    "- play arm A forever.\n",
    "\n",
    "Questions: Understand the code below and\n",
    "1. Show that, if $\\tau$ is fixed, then the (expected) regret after $T$ rounds grows linearly with $T$ when $T \\to +\\infty$ (which is very bad). Explain the title of this section.\n",
    "2. Compare the above conclusion with the theoretical result stating that the regret is only logarithmic in $T$ provided $m$ is chosen roughly like $m \\approx \\log(T \\Delta^2)/\\Delta^2$, where $\\Delta = |\\mu_0 - \\mu_1|$.\n",
    "3. Try different values for the parameters $m$, $\\mu_0$, and $\\mu_1$. Interpret the results in terms of exploration/exploitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 50\n",
    "tau = 2*m # size of exploration phase\n",
    "T = 10**4  # total number of rounds\n",
    "MC = 200    # number of Monte-Carlo experiments\n",
    "mu = [0.6,0.4]\n",
    "# Try also:\n",
    "#mu = [0.8,0.2]\n",
    "#mu = [0.51,0.49]\n",
    "mustar = max(mu)\n",
    "\n",
    "avregret = zeros(T)  # will contain an estimate of the (expected) regret R_t for all t<=T\n",
    "\n",
    "for experiment in range(MC):\n",
    "    regret = 0\n",
    "    muhat = [0,0] # after exploration phase, will contain estimates of expected rewards of arms 0 and 1\n",
    "    # exploration phase\n",
    "    for t in range(tau):\n",
    "        A = t % 2 # we choose 0 or 1 alternatively\n",
    "        r = binomial(1,mu[A]) # observed reward\n",
    "        muhat[A] += r/m\n",
    "        regret += mustar - r\n",
    "        avregret[t] += regret/MC # updates the estimate of the expected regret at time t\n",
    "    # exploitation phase: play best arm forever\n",
    "    A = argmax(muhat)\n",
    "    for t in range(tau,T):\n",
    "        r = binomial(1,mu[A]) # observed reward\n",
    "        regret += mustar - r\n",
    "        avregret[t] += regret/MC # update the estimate of the (expected) regret at time t \n",
    "    \n",
    "plot(avregret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The $\\epsilon$-greedy algorithm.\n",
    "\n",
    "The \"$\\epsilon$-greedy algorithm\" proceeds as follows:\n",
    "- For the first two iterations: play actions $0$ and $1$ once\n",
    "- At each iteration $t=3,4,5,\\ldots$,\n",
    "    - with probability $1-\\epsilon_t$, play the empirically best arm\n",
    "    - with probability $\\epsilon_t$, play action $0$ or $1$ uniformly at random\n",
    "\n",
    "Questions: Implement the algorithm and\n",
    "1. Show that, if $\\epsilon_t = \\min \\left\\{\\frac{c}{t},1\\right\\}$ with a large enough constant $c>0$, then the (expected) regret after $T$ rounds only grows logarithmically when $T \\to +\\infty$.\n",
    "2. What happens if we choose $\\epsilon_t=\\epsilon$ constant over time? or too small a value for $c$? (interpret the results in terms of exploration/exploitation)\n",
    "3. What is the main advantage of $\\epsilon$-greedy compared to Explore-Then-Commit? Which drawback remains?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 5*10**3  # total number of rounds\n",
    "MC = 500    # number of Monte-Carlo experiments\n",
    "mu = [0.6,0.4]\n",
    "# Try also:\n",
    "#mu = [0.8,0.2]\n",
    "#mu = [0.51,0.49]\n",
    "mustar = max(mu)\n",
    "\n",
    "# exploration parameter of the algorithm: epsilon_t = min(c/t,1) where\n",
    "c = 6*2/abs(mu[0]-mu[1])**2\n",
    "# (the choice of any c > 5*K/\\Delta^2 is guaranteed by the theory to yield logarithmic regret)\n",
    "\n",
    "avregret = zeros(T)  # will contain an estimate of the (expected) regret R_t for all t<=T\n",
    "\n",
    "for experiment in range(MC):\n",
    "    regret = 0\n",
    "    muhat = [0,0] # after exploration phase, will contain estimates of expected rewards of arms 0 and 1\n",
    "    armcounts = [0,0] # number of draws for each arm\n",
    "    # initialization (dates 0 et 1)\n",
    "    for t in range(2):\n",
    "    # exploration-exploitation phase: play epsilon-greedy\n",
    "    for t in range(2,T):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The UCB algorithm.\n",
    "\n",
    "The UCB-algorithm proceeds as follows:\n",
    "- For the first two iterations: play actions $0$ and $1$ once.\n",
    "- At each iteration $t=3,4,5,\\ldots$, play the arm that maximizes the Upper Confidence Bound:\n",
    "$$I_t \\in \\rm{argmax}_{a \\in \\{0,1\\}} \\left\\{\\hat{\\mu}_{t-1}(a) + \\sqrt{\\frac{2 \\log(t)}{T_a(t-1)}} \\;\\right\\}$$\n",
    "   \n",
    "Questions: Implement the algorithm and\n",
    "1. Show that the (expected) regret after $T$ rounds also grows logarithmically when $T \\to +\\infty$.\n",
    "2. Compare UCB with the first two algorithms in terms of performances and of required prior knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 5*10**3  # total number of rounds\n",
    "MC = 500    # number of Monte-Carlo experiments\n",
    "mu = [0.6,0.4]\n",
    "# Try also:\n",
    "#mu = [0.8,0.2]\n",
    "#mu = [0.51,0.49]\n",
    "mustar = max(mu)\n",
    "\n",
    "avregret = zeros(T)  # will contain an estimate of the (expected) regret R_t for all t<=T"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
